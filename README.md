Hereâ€™s a clear, beginner-friendly `README.md` for your RAG project, designed to explain what it does, how it works, and how someone can run it from scratch.

---

## ğŸ“„ `README.md`

````markdown
# ğŸ§  RAG-Food: Simple Retrieval-Augmented Generation with ChromaDB + Ollama

This is a **minimal working RAG (Retrieval-Augmented Generation)** demo using:

- âœ… Local LLM via [Ollama](https://ollama.com/)
- âœ… Local embeddings via `mxbai-embed-large`
- âœ… [ChromaDB](https://www.trychroma.com/) as the vector database
- âœ… A simple food dataset in JSON (Indian foods, fruits, etc.)

---

## ğŸ¯ What This Does

This app allows you to ask questions like:

- â€œWhich Indian dish uses chickpeas?â€
- â€œWhat dessert is made from milk and soaked in syrup?â€
- â€œWhat is masala dosa made of?â€

It **does not rely on the LLMâ€™s built-in memory**. Instead, it:

1. **Embeds your custom text data** (about food) using `mxbai-embed-large`
2. Stores those embeddings in **ChromaDB**
3. For any question, it:
   - Embeds your question
   - Finds relevant context via similarity search
   - Passes that context + question to a local LLM (`llama3.2`)
4. Returns a natural-language answer grounded in your data.

---

## ğŸ“¦ Requirements

### âœ… Software

- Python 3.8+
- Ollama installed and running locally
- ChromaDB installed

### âœ… Ollama Models Needed

Run these in your terminal to install them:

```bash
ollama pull llama3.2
ollama pull mxbai-embed-large
````

> Make sure `ollama` is running in the background. You can test it with:
>
> ```bash
> ollama run llama3.2
> ```

---

## ğŸ› ï¸ Installation & Setup

### 1. Clone or download this repo

```bash
git clone https://github.com/yourname/rag-food
cd rag-food
```

### 2. Install Python dependencies

```bash
pip install chromadb requests
```

### 3. Run the RAG app

```bash
python rag_run.py
```

If it's the first time, it will:

* Create `foods.json` if missing
* Generate embeddings for all food items
* Load them into ChromaDB
* Run a few example questions

---

## ğŸ“ File Structure

```
rag-food/
â”œâ”€â”€ rag_run.py       # Main app script
â”œâ”€â”€ foods.json       # Food knowledge base (created if missing)
â”œâ”€â”€ README.md        # This file
```

---

## ğŸ§  How It Works (Step-by-Step)

1. **Data** is loaded from `foods.json`
2. Each entry is embedded using Ollama's `mxbai-embed-large`
3. Embeddings are stored in ChromaDB
4. When you ask a question:

   * The question is embedded
   * The top 1â€“2 most relevant chunks are retrieved
   * The context + question is passed to `llama3.2`
   * The model answers using that info only

---

## ğŸ” Try Custom Questions

You can update `rag_run.py` to include your own questions like:

```python
print(rag_query("What is tandoori chicken?"))
print(rag_query("Which foods are spicy and vegetarian?"))
```

---

## ğŸš€ Next Ideas

* Swap in larger datasets (Wikipedia articles, recipes, PDFs)
* Add a web UI with Gradio or Flask
* Cache embeddings to avoid reprocessing on every run

---

## ğŸ‘¨â€ğŸ³ Credits

Made by Callum using:

* [Ollama](https://ollama.com)
* [ChromaDB](https://www.trychroma.com)
* [mxbai-embed-large](https://ollama.com/library/mxbai-embed-large)
* Indian food inspiration ğŸ›

